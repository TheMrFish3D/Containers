# =============================================
# NVIDIA Triton Inference Server
#
# Service:
# - triton â€” NVIDIA Triton Inference Server for multi-framework model serving
# Docker Hub:
# - https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver
# GitHub:
# - https://github.com/triton-inference-server/server
#
# Volumes:
# - /fileServerMountPath/data/models -> /models (model repository)
#
# How to use:
# 1) Place models in /fileServerMountPath/data/models following Triton repo structure.
#    Set /fileServerMountPath to your Linux host path. On Docker Desktop + WSL, use /mnt/<drive> (e.g., /mnt/d).
# 2) Start:  docker compose up -d
# 3) Stop:   docker compose down
# 4) Update: docker compose pull && docker compose up -d
# 5) Endpoints: REST 8000, gRPC 8001, Metrics 8002
# =============================================
version: "3.9"
services:
  triton:
    image: nvcr.io/nvidia/tritonserver:${TRITON_TAG:-24.12}-py3
    container_name: triton
    restart: unless-stopped
    runtime: nvidia
    shm_size: "2g"
    ulimits:
      memlock: -1
      stack: 67108864
    environment:
      - TZ=Australia/Brisbane
    ports:
      - "8000:8000"
      - "8001:8001"
      - "8002:8002"
    volumes:
      - /path/to/your/mount/data/models:/models
    command: tritonserver --model-repository=/models --model-control-mode=poll --repository-poll-secs=60 --strict-model-config=false --exit-on-error=false
